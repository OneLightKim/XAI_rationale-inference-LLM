{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tqdm import tqdm\n",
                "\n",
                "def create_example(all_data):\n",
                "    all_result = []\n",
                "    for data in tqdm(all_data):\n",
                "        data_id = data[\"_id\"]\n",
                "        Question = data[\"question\"]\n",
                "        answer = data[\"answer\"]\n",
                "        context = data[\"context\"]\n",
                "        sent_list = []\n",
                "        sentences = \"\"\n",
                "        for index, j in enumerate(context):\n",
                "            title = j[0]\n",
                "            sent_list.append(title.strip()+ \".\")\n",
                "            if sentences == \"\":\n",
                "                sentences = title + \". \"\n",
                "            else:\n",
                "                sentences = sentences + \" \" + title + \". \"\n",
                "            for sent in j[1]:\n",
                "                sentences = sentences + sent\n",
                "                sent_list.append(sent.strip())\n",
                "            \n",
                "        result = {}\n",
                "        result[\"_id\"] = data_id\n",
                "        result[\"question\"] = Question\n",
                "        result[\"document\"] = sentences\n",
                "        result[\"sent\"] = sent_list\n",
                "        result[\"output\"] = answer\n",
                "\n",
                "        all_result.append(result)\n",
                "\n",
                "    return all_result\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 90447/90447 [00:02<00:00, 32040.75it/s]\n"
                    ]
                }
            ],
            "source": [
                "import json\n",
                "file_path = \"../data/origin/hotpot_train_v1.1_re.json\"\n",
                "# file_path = \"../data/origin/hotpot_dev.json\"\n",
                "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
                "    dev_data = json.load(file)\n",
                "\n",
                "input_data = create_example(dev_data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/rbqlsquf2/workspace/XAI_rationale-inference-LLM/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                }
            ],
            "source": [
                "from transformers import AutoTokenizer\n",
                "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        " 20%|██        | 18175/90447 [01:14<05:01, 239.45it/s]"
                    ]
                }
            ],
            "source": [
                "from tqdm import tqdm\n",
                "all_len = []\n",
                "all_result = []\n",
                "\n",
                "for example in tqdm(input_data):\n",
                "    MAX_LENGTH = 2048\n",
                "    input_ids, attention_mask, labels = [], [], []\n",
                "    example[\"document\"] = example[\"document\"].strip()\n",
                "    # token 된 doc\n",
                "    token_doc = {\"input_ids\": [], \"attention_mask\": []}\n",
                "    # document 문장 index\n",
                "    sentence_number = 0\n",
                "    sentence_position = []\n",
                "    for i, sent in enumerate(example[\"sent\"]):\n",
                "        # 0번 문장은 instruction으로 지정할 계획\n",
                "        sent = sent.strip()\n",
                "        token_sent = tokenizer(sent + \" \", add_special_tokens=False)\n",
                "        sentence_number += 1  # 1부터 시작\n",
                "        sentence_position.extend([sentence_number] * len(token_sent[\"input_ids\"]))\n",
                "        token_doc[\"input_ids\"] += token_sent[\"input_ids\"]\n",
                "        token_doc[\"attention_mask\"] += token_sent[\"attention_mask\"]\n",
                "    token_end = tokenizer(\"<|im_end|>\\n\", add_special_tokens=False)\n",
                "    sentence_position.extend([0] * len(token_end))\n",
                "    token_doc[\"input_ids\"] += token_end[\"input_ids\"]\n",
                "    token_doc[\"attention_mask\"] += token_end[\"attention_mask\"]\n",
                "    instruction = tokenizer(\n",
                "        f\"<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n**Question:{example['question']}\\n**Document:\\n\",\n",
                "        add_special_tokens=False,\n",
                "    )\n",
                "    response = tokenizer(\n",
                "        f\"<|im_start|>assistant\\n**Answer:{example['output'].strip()}<|im_end|>\\n\", add_special_tokens=False\n",
                "    )\n",
                "    \n",
                "    input_ids = instruction[\"input_ids\"] + token_doc[\"input_ids\"] + response[\"input_ids\"]\n",
                "    count = len(input_ids)\n",
                "    if count <= MAX_LENGTH:\n",
                "        all_result.append(example)\n",
                "    all_len.append(count)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "90447\n",
                        "86434\n"
                    ]
                }
            ],
            "source": [
                "print(len(all_result))\n",
                "print(sum(all_len)/len(all_len))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "file_path = \"../1112data/hotpot_train.json\"\n",
                "with open(file_path, 'w', encoding='utf-8') as f:\n",
                "    json.dump(all_result, f, ensure_ascii=False, indent=4)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open(\"../data/1008data/hotpot_train.json\", \"w\", encoding=\"utf-8\") as f:\n",
                "    json.dump(all_result, f, ensure_ascii=False, indent=4)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 7405/7405 [00:18<00:00, 405.87it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "668\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "from tqdm import tqdm\n",
                "all_len = []\n",
                "all_result = []\n",
                "over_num = 0\n",
                "for input_data_ in tqdm(input_data):\n",
                "    text = input_data_[\"all_text\"]\n",
                "    count = len(tokenizer(text)[\"input_ids\"])\n",
                "    if count <= 2048:\n",
                "        all_result.append(input_data_)\n",
                "    else:\n",
                "        over_len = count - 2048\n",
                "        input_data_['text'] = input_data_['text'][:over_len]\n",
                "        over_num +=1\n",
                "    # all_len.append(len(tokenizer(text)[\"input_ids\"]))\n",
                "print(over_num)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 7405/7405 [00:00<00:00, 1753249.85it/s]\n"
                    ]
                }
            ],
            "source": [
                "for input_data_ in tqdm(input_data):\n",
                "    del input_data_[\"all_text\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open(\"../data/qwen_hotpot_test_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
                "    json.dump(all_result, f, ensure_ascii=False, indent=4)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 90447/90447 [04:12<00:00, 357.70it/s]\n"
                    ]
                }
            ],
            "source": [
                "from tqdm import tqdm\n",
                "all_len = []\n",
                "all_result = []\n",
                "\n",
                "for input_data_ in tqdm(input_data):\n",
                "    text = input_data_[\"text\"]\n",
                "    if len(tokenizer(text)[\"input_ids\"]) <= 2048:\n",
                "        # data[\"text\"] = data[\"text\"]\n",
                "        all_result.append(input_data_)\n",
                "    # all_len.append(len(tokenizer(text)[\"input_ids\"]))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open(\"data/qwen_hotpot_train_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
                "    json.dump(all_result, f, ensure_ascii=False, indent=4)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open(\"data/teddst_dev.json\", \"w\", encoding=\"utf-8\") as f:\n",
                "    json.dump(input_data, f, ensure_ascii=False, indent=4)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "**Answer**: yes\n",
                        "**Supporting Sentences**: [4] Scott Derrickson (born July 16, 1966) is an American director, screenwriter and producer.\n",
                        "[17] Edward Davis Wood Jr. (October 10, 1924 – December 10, 1978) was an American filmmaker, actor, writer, producer, and director.\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "print(input_data[0][\"label\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "6539\n"
                    ]
                }
            ],
            "source": [
                "count = len(list(filter(lambda x: x < 2048, all_len)))\n",
                "print(count)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
